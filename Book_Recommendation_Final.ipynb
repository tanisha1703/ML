{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AniketGhorpade/Book_Recommendation/blob/main/Book_Recommendation_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name :- Book Recommendation System**\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Unsupervised\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member 1** - Aniket K. Ghorpade"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are three different datasets present originally in this project, which includes \"books,users and ratings\". Further detailed information about the datasets is provided below. We were working on a unsupervised machine learning problem here. Our main task in this project was to build a recommendation system which will be able to recommend various books to our users on the basis of ratings provided by them.\n",
        "\n",
        "Intially we have performed data cleanig and data transformation to bring available data in desirable format. Then we have performed sevral data transformations and derived few new features so that it will be easier to do exploratory analysis. After that we have perfomrmed exploratory data analysis respectively we did univariate analysis, bia-variate analysis to check relationship between different variables followed by multi-variate analysis, so it gave us various insights which are useful to take business decisions. With all above procedure we have finished our exploratory data analysis part of this project.\n",
        "\n",
        "Then we have performed hypothesis testing to check various hypothetical statements by using various statistical methods. \n",
        "\n",
        "After all above procedure we have moved towards machine learning part of our project in which we have made our data model ready by doing various transformations such as label encoding, outlier handlings etc. Then we have decided to use collaborative filtering in this project which are suitable in our case as per the objective of our project. This method makes automatic predictions (filtering) about the interests of a user by collecting preferences or taste information from many users (collaborating). The underlying assumption of the collaborative filtering approach is that if a person A has the same opinion as a person B on a set of items, A is more likely to have B's opinion for a given item than that of a randomly chosen person.\n",
        "\n",
        "We have used Model Based approach in this project and we have used Latent Factor Model :- Singular Value Decomposition (SVD) to achieve our objective, we are able to predict closet top 5 users with recall rate of almost 20%."
      ],
      "metadata": {
        "id": "hWkfPqmil2hL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/AniketGhorpade/Book_Recommendation"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**During the last few decades, with the rise of Amazon,YouTube,Netflix and many other such web services, recommend-er systems have taken more and more place in our lives. From e-commerece (suggest to buyers articles that could interest them) to online advertisement(suggest to users the right contents matching there preferences), recommender systems are today unavoidable in our daily online journeys.\n",
        "  In a very genral way, recommender systems are algorithms aimed at suggesting relevant.\n",
        "  Items to users(Items being movies to watch, text to read, products to buy or anything else in the industries.) Recommender systems are really critical in some industries as they can genrate a huge amount of income when they are efficient or also be a way to stand out significantly from competitiors. The main objective is to create a book recommendation system for users.**"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "_uKY3ic1EQ6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "\n",
        "## Data handling libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "from scipy import stats\n",
        "import math\n",
        "import random\n",
        "import sklearn\n",
        "\n",
        "## Data visualisation libraries\n",
        "\n",
        "import matplotlib.pyplot as plt \n",
        "import seaborn as sns \n",
        "from matplotlib import colors\n",
        "from matplotlib.ticker import PercentFormatter\n",
        "\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import missingno as msno\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "## Importing necessary libraries for creating recommendation stystem\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.sparse.linalg import svds"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "\n",
        "books = pd.read_csv('/content/drive/MyDrive/Capstone Projects Submission/4. Unsupervised ML/Book Recommendation/Books.csv')\n",
        "ratings = pd.read_csv('/content/drive/MyDrive/Capstone Projects Submission/4. Unsupervised ML/Book Recommendation/Ratings.csv')\n",
        "users =  pd.read_csv('/content/drive/MyDrive/Capstone Projects Submission/4. Unsupervised ML/Book Recommendation/Users.csv') "
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "books.head(3)"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratings.head(3) "
      ],
      "metadata": {
        "id": "VyrJa8okF9Ol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users.head(3)"
      ],
      "metadata": {
        "id": "a-t8gT-bGBUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [books,ratings,users]"
      ],
      "metadata": {
        "id": "l0zwGvOGG288"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "for i in data:\n",
        "  print(i.shape)"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "books.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratings.info()"
      ],
      "metadata": {
        "id": "nJPf30LwIDSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users.info()"
      ],
      "metadata": {
        "id": "-9e4HC_bIH1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "books[books.duplicated()]"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users[users.duplicated()]"
      ],
      "metadata": {
        "id": "1ae75u0gIglk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratings[ratings.duplicated()] "
      ],
      "metadata": {
        "id": "qB7HbylSIlnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No any duplicate value is present in the dataset."
      ],
      "metadata": {
        "id": "j42oAKWHI1LY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "books.isna().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "plt.figure(figsize=(7,3))\n",
        "msno.bar(books)"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users.isna().sum()"
      ],
      "metadata": {
        "id": "6rNLBKFrNZDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(7,3))\n",
        "msno.bar(users)"
      ],
      "metadata": {
        "id": "jT08VD11NdPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratings.isna().sum()"
      ],
      "metadata": {
        "id": "8DHPr1_VN8BE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(7,3))\n",
        "msno.bar(ratings)"
      ],
      "metadata": {
        "id": "5e7hXqVLOP8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Users: Contains the users. Note that the user ID's have been anonymized and map to integers. Demographic data is provided (Location,Age) is avaiable. Otherwise this field contain null values.\n",
        "\n",
        "2. Books: books are identified by there respective ISBN. Invalid ISBN's have already been removed from the dataset. Moreover some content based information is given (Book author, Book title, Year of publication, publisher) obatined from Amazon Web Services. Note that in the case of several authors, only the first is provided. URL's linking to cover images are also given, apperaing in three different flavours (Image URL-S, Image URL-L,Image URL-M).i.e. small,medium and large. This URL's point to the amazon website.\n",
        "\n",
        "3. Ratings: Contains the book ratings information. Ratings(Book rating) are either explicit, expressed on a scale from 1-10 (Higher values denoting higher appreciation), or implicit expressed by O.\n",
        "\n"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Books Dataset**"
      ],
      "metadata": {
        "id": "wGtcurPuVf1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "books.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "\n",
        "books.describe(include='all')"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description "
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. There are around 270,000 records available in the dataset of books out of that all the records has unique ISBN number.\n",
        "2. Out of 271360 books 242,135 books has unique title and selected poems is the title used majority of the times that is 27.\n",
        "3. We have data of various books written by around 102023 various writers, out of that Agatha Christie has 632 books.\n",
        "4. We have books for around 2 centuries and out of that most of the books come from year 2002.\n",
        "5. Around 16800 unique publishers books data is stored in the dataset, Harlequin publishers have majority of the books that is around 7535 published by them."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Users Dataset**"
      ],
      "metadata": {
        "id": "bXPEZH2qVrW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset columns\n",
        "\n",
        "users.columns"
      ],
      "metadata": {
        "id": "sGaLYQ3RUngO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset describe\n",
        "\n",
        "users.describe(include='all')"
      ],
      "metadata": {
        "id": "K7cWvNV2Uctn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description "
      ],
      "metadata": {
        "id": "eHhW9dAaV_7B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. User id is unique id for each user.\n",
        "2. There are around 57000 various loacations to which our user belongs to, out of that London is the place to which most of our users come from.\n",
        "3. Age columns is showing, age of our users mean of age is 34 and distribution is looking positively skewed."
      ],
      "metadata": {
        "id": "R0VY0lDMU3-5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ratings Dataset**"
      ],
      "metadata": {
        "id": "rUN7KzTVV1_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset columns\n",
        "\n",
        "ratings.columns"
      ],
      "metadata": {
        "id": "oxb50oVdVNXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset describe\n",
        "\n",
        "ratings.describe(include='all')"
      ],
      "metadata": {
        "id": "j7L6uPH0VTwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description "
      ],
      "metadata": {
        "id": "1xwazLIDWDdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ratings daatset showing ratings from 1 to 10, given by various users to different books available in the dataset. Distiribution is looking negatively skewed as our median of ratings is less than mean."
      ],
      "metadata": {
        "id": "AfykWqfEdqHe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***3. Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "users.head(3)"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_loc(tx):\n",
        "  txt = tx.split(\", \")\n",
        "  return(txt)"
      ],
      "metadata": {
        "id": "DcM0taTSiguZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_city(tx):\n",
        "  txt = tx[0]\n",
        "  return(txt)"
      ],
      "metadata": {
        "id": "V2dhM_kbmZWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_region(tx):\n",
        "  if len(tx) > 1:\n",
        "    txt = tx[1]\n",
        "  else:\n",
        "    txt = 'None'\n",
        "  return(txt)"
      ],
      "metadata": {
        "id": "AHzUZFGzjy4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_country(tx):\n",
        "  if len(tx) == 3:\n",
        "    txt = tx[2]\n",
        "  else:\n",
        "    txt = 'None'\n",
        "  return(txt)"
      ],
      "metadata": {
        "id": "CsPF_88xj8kt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users['loc'] = users[\"Location\"].apply(lambda x: split_loc(x))\n",
        "users['city'] = users[\"loc\"].apply(lambda x: split_city(x))\n",
        "users['region'] = users[\"loc\"].apply(lambda x: split_region(x))\n",
        "users['country'] = users[\"loc\"].apply(lambda x: split_country(x))"
      ],
      "metadata": {
        "id": "HVkmJg8Li_YN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users = users.drop('loc',axis=1)\n",
        "users.head(3)"
      ],
      "metadata": {
        "id": "8gkLdLpmlaUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. We have splitted location column into three different features they are city, region and country so that we could do further analysis by using them."
      ],
      "metadata": {
        "id": "l3uRtBQ4pJlZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "books.head(3)"
      ],
      "metadata": {
        "id": "IrHlySrfpguh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "books.drop(['Image-URL-S','Image-URL-M','Image-URL-L'],axis=1,inplace=True)\n",
        "books.head(3)"
      ],
      "metadata": {
        "id": "TXZmD9Avpy27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. We have dropped url columns from books dataset as we will not be considering them in further analysis."
      ],
      "metadata": {
        "id": "Ae7reMXUqJN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ratings.head(4)"
      ],
      "metadata": {
        "id": "zxxZiJR_qcUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "users.head(3)"
      ],
      "metadata": {
        "id": "vCRSqRuoEVI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.core.dtypes.cast import Sized\n",
        "# Chart - 1 visualization code of bar chart\n",
        "\n",
        "def barplot_fun(size,Data,X,Y,col='deepskyblue'):\n",
        "    '''This function returns barplot for particular variable'''\n",
        "\n",
        "    plt.figure(figsize=size)\n",
        "\n",
        "    sns.set(style=\"whitegrid\", color_codes=True)\n",
        "    plots = sns.barplot(x=X, y=Y, data=Data,color=col)\n",
        "    plots.set_xticklabels(plots.get_xticklabels(), rotation=30, ha=\"right\",size=12)\n",
        "    \n",
        "    # Iterating over the bars one-by-one\n",
        "    for bar in plots.patches:\n",
        "        plots.annotate(format(bar.get_height(), '.0f'),\n",
        "                      (bar.get_x() + bar.get_width() / 2,\n",
        "                        bar.get_height()), ha='center', va='center',\n",
        "                      size=14, xytext=(0, 8),\n",
        "                      textcoords='offset points')\n",
        "    \n",
        "    # Setting the label for x-axis\n",
        "    plt.xlabel(X, size=14)\n",
        "    \n",
        "    # Setting the label for y-axis\n",
        "    plt.ylabel(Y, size=14)\n",
        "    \n",
        "    # Setting the title for the graph\n",
        "    plt.title(X+\" Distribution\",size=16)\n",
        "    \n",
        "    # Finally showing the plot\n",
        "    plt.show()\n",
        "     "
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def value_count(Data,X):\n",
        "    \n",
        "    ''' This code returns datframe with value counts'''\n",
        "\n",
        "    out = pd.DataFrame(Data[X].value_counts().reset_index())\n",
        "    out.rename(columns = {'index':X,X:'count'}, inplace = True)\n",
        "    return(out)"
      ],
      "metadata": {
        "id": "K6oc3c83HKeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Which are the countrys our users belongs to? Do analysis on the basis of locality of our users."
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "users_country = value_count(Data=users,X=\"country\")\n",
        "users_country = users_country.head(10)"
      ],
      "metadata": {
        "id": "oiK_ym6eHAgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "barplot_fun(size=(14, 6),Data=users_country,X='country',Y='count',col='blue')"
      ],
      "metadata": {
        "id": "1nDK4lW6GYww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Most of our users belongs to united states, followed by canada and then european countries such as united kingdom, germany, spain etc.\n",
        "2. Need to focus on asian and african countries also as we have very low user base in this countries."
      ],
      "metadata": {
        "id": "4kjtGCwoLegA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Which are the regions most of our users come from?"
      ],
      "metadata": {
        "id": "APXmbGrjMTF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "users_region = value_count(Data=users,X=\"region\")\n",
        "users_region = users_region.head(10)"
      ],
      "metadata": {
        "id": "3Zkx045CLdbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "barplot_fun(size=(14, 6),Data=users_region,X='region',Y='count',col='deepskyblue') "
      ],
      "metadata": {
        "id": "Vg5rfDOENGuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Most of our users belongs to devoloped regions such as california, england, ontario, texas, new york.\n",
        "2. Need to focus regions which are not much devolped or say rural in nature, need to expand connectivity so that we could reach to as many as users we can by going beyond urbanisation."
      ],
      "metadata": {
        "id": "D2NBaSnrNr-B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Show top cities with most number of book readers as per our dataset?"
      ],
      "metadata": {
        "id": "IWWd10PAOqcS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "users_city = value_count(Data=users,X=\"city\")\n",
        "users_city = users_city.head(10)"
      ],
      "metadata": {
        "id": "lmXsT5qHO-W6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "barplot_fun(size=(14, 6),Data=users_city,X='city',Y='count',col='green') "
      ],
      "metadata": {
        "id": "EIwgPor9PCDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Above graph is showing top cities to which our users come from.\n",
        "2. Uropean cities are at top in all of them."
      ],
      "metadata": {
        "id": "0LItEROzPghf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code for plotting histogram of contineous variable"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def distr(DF,x,n_bins=10):\n",
        "\n",
        "  ''' This function gives univariate distribution of contineous variable in the form of Histogram '''\n",
        "\n",
        "  legend = ['distribution']\n",
        "\n",
        "  # Creating histogram\n",
        "  fig, axs = plt.subplots(1, 1,\n",
        "                          figsize =(8,5),\n",
        "                          tight_layout = True)\n",
        "  \n",
        "  \n",
        "  # Remove axes splines\n",
        "  for s in ['top', 'bottom', 'left', 'right']:\n",
        "      axs.spines[s].set_visible(False)\n",
        "  \n",
        "  # Remove x, y ticks\n",
        "  axs.xaxis.set_ticks_position('none')\n",
        "  axs.yaxis.set_ticks_position('none')\n",
        "    \n",
        "  # Add padding between axes and labels\n",
        "  axs.xaxis.set_tick_params(pad = 5)\n",
        "  axs.yaxis.set_tick_params(pad = 10)\n",
        "  \n",
        "  # Add x, y gridlines\n",
        "  axs.grid(b = True, color ='grey',\n",
        "          linestyle ='-.', linewidth = 0.5,\n",
        "          alpha = 0.6)\n",
        "  kur = round(DF[x].kurt(),2)\n",
        "  ske = round(DF[x].skew(),2)\n",
        "  # Add Text watermark\n",
        "  fig.text(0.9, 0.75, 'Kurtosis: '+str(kur)+' & Skewness: '+str(ske),\n",
        "          fontsize = 14,\n",
        "          color ='black',\n",
        "          ha ='right',\n",
        "          va ='bottom',\n",
        "          alpha = 1.0)\n",
        "  \n",
        "  # Creating histogram\n",
        "  N, bins, patches = axs.hist(DF[x], bins = n_bins)\n",
        "  \n",
        "  # Setting color\n",
        "  fracs = ((N**(1 / 5)) / N.max())\n",
        "  norm = colors.Normalize(fracs.min(), fracs.max())\n",
        "  \n",
        "  for thisfrac, thispatch in zip(fracs, patches):\n",
        "      color = plt.cm.viridis(norm(thisfrac))\n",
        "      thispatch.set_facecolor(color)\n",
        "  \n",
        "  # Adding extra features   \n",
        "  plt.xlabel(x,size=14)\n",
        "  plt.ylabel(\"Count\",size=14)\n",
        "  plt.legend(legend)\n",
        "  plt.title('Distribution Of '+x,size=16)\n",
        "  \n",
        "  # Show plot\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "FrqSW7lCTURR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Show distribution of age of user by using histogram"
      ],
      "metadata": {
        "id": "v2m5uam6VKqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distr(DF=users,x='Age',n_bins=20)\n",
        "print(f'Avg. age of users : {round(users.Age.mean(),2)}')\n",
        "print(f'Median age of users : {round(users.Age.median(),2)}')"
      ],
      "metadata": {
        "id": "CCKoUjUcUoXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. With kurtosis 6.04 we could say that there is high peakedness in the data, mean is representing data very well kurtosis is highly positive. \n",
        "2. With skewness 1.18 it is clearly showing positively skewed distribution, most of the users age lies between 24-40 which is clearly showing that we have established good connectivity with youth."
      ],
      "metadata": {
        "id": "ViIkyeOEWAjI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "books.head(3)"
      ],
      "metadata": {
        "id": "fDTS6Cu8Y3zP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Which book titles are repeatatively used by authors? Which book titles are showing different book series?"
      ],
      "metadata": {
        "id": "IDFQcM3Sa8c7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "books_title = value_count(Data=books,X=\"Book-Title\")\n",
        "books_title = books_title.head(10) "
      ],
      "metadata": {
        "id": "pBAttZKqbUuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "barplot_fun(size=(14, 6),Data=books_title,X='Book-Title',Y='count',col='brown')"
      ],
      "metadata": {
        "id": "o2xXaLCBboEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above books belongs to different book series or we could say above are few of the book series of which information is available in the dataset."
      ],
      "metadata": {
        "id": "YqsO882Ocez-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Number of books published w.r.t year as per available dataset."
      ],
      "metadata": {
        "id": "jFRILMNcdjsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "books_year = pd.DataFrame(books['Year-Of-Publication'].value_counts())\n",
        "books_year.drop(['DK Publishing Inc','Gallimard'],axis=0,inplace=True)\n",
        "books_year = books_year.reset_index()\n",
        "books_year.rename(columns = {'index':'Year','Year-Of-Publication':'count'}, inplace = True)\n",
        "books_year['Year'] = books_year['Year'].astype(int)\n",
        "books_year.sort_values(by='Year',inplace=True)\n",
        "books_year.reset_index(inplace=True)\n",
        "books_year.drop('index',axis=1,inplace=True)\n",
        "books_year = books_year.iloc[6:,0:]\n",
        "books_year.head(5)"
      ],
      "metadata": {
        "id": "gEFGSTL0d8NG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = books_year['Year']\n",
        "y = books_year['count']\n",
        "\n",
        "plt.figure(figsize=(14, 4))\n",
        "\n",
        "\n",
        "plt.plot(x,y,color='red')\n",
        "plt.xlabel(\"Year\",size=14)  # add X-axis label\n",
        "plt.ylabel(\"Total books published\",size=14)  # add Y-axis label\n",
        "plt.title(\"Books published per year\",size=16)  # add title\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o32syA3BkIEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. As we could see in above graph, from 1900 to 1960 published books with respect to years were very low in number as per available dataset.\n",
        "2. From 1970 onwards to 2000 trend in books publication is increased and ample amount of books that are published in this time span are available in the dataset.\n",
        "3. From 2010 onwards again this number has reduced as per the information that we have got.\n",
        "4. We could simply conclude that in our dataset, we have information about the books which are published in the timespan of 1975 to 2005."
      ],
      "metadata": {
        "id": "CX3BMrF4mSuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Authors with most number of books as per available dataset."
      ],
      "metadata": {
        "id": "a4II1LqK3MO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "books_author = value_count(Data=books,X=\"Book-Author\")\n",
        "books_author = books_author.head(10) "
      ],
      "metadata": {
        "id": "R4M_ddaw5GcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "barplot_fun(size=(14, 6),Data=books_author,X='Book-Author',Y='count',col='lightgreen')"
      ],
      "metadata": {
        "id": "PLnApJEE5WJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above are few of the authors, which has most number of books published as per available dataset."
      ],
      "metadata": {
        "id": "545Apsr45733"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Top publishers in the dataset."
      ],
      "metadata": {
        "id": "IzmhdKqn7lBW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "books_publisher = value_count(Data=books,X=\"Publisher\")\n",
        "books_publisher = books_publisher.head(10) "
      ],
      "metadata": {
        "id": "10aGY-7475J2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "barplot_fun(size=(14, 6),Data=books_publisher,X='Publisher',Y='count',col='darkorange')"
      ],
      "metadata": {
        "id": "deW9z3Vo8Lba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above are few of the publishers, which has most number of books published as per available dataset."
      ],
      "metadata": {
        "id": "o3t9vCAF8mx9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Show distribution of book ratings."
      ],
      "metadata": {
        "id": "yn5i4zrCB14M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distr(DF=ratings,x='Book-Rating',n_bins=10)\n",
        "print(f'Avg. book-rating : {round(ratings[\"Book-Rating\"].mean(),2)}')\n",
        "print(f'Median of book-rating : {round(ratings[\"Book-Rating\"].median(),2)}')"
      ],
      "metadata": {
        "id": "UO625GtwDEgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. With kurtosis -1.24 we could say that there is no any peakedness in the data, mean is definately not representing data, distribution is platykurtic. \n",
        "2. With skewness 0.73 it is clearly showing slightly positively skewed distribution median is very low than mean of book-rating, 50 % of the peoples has given 0 rating to most of the books."
      ],
      "metadata": {
        "id": "SEoAkw_mEdFz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Top books as per rating."
      ],
      "metadata": {
        "id": "qFKEztP2F_i4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ratings.head(5)"
      ],
      "metadata": {
        "id": "SeV9GdiDB7cT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_book_rating = pd.DataFrame(ratings.groupby('ISBN')['Book-Rating'].mean())\n",
        "top_book_rating = top_book_rating.reset_index()\n",
        "top_book_rating.rename(columns = {'index':'ISBN'}, inplace = True)\n",
        "top_book_rating.sort_values(by='Book-Rating',ascending=False,inplace=True)\n",
        "top_book_rating.reset_index(inplace=True)\n",
        "top_book_rating.drop('index',axis=1,inplace=True)\n",
        "top_book_rating "
      ],
      "metadata": {
        "id": "Z9j7E3_FF4VZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In above dataframe we are getting average ratings for various books available in the dataset w.r.t there ISBN numbers."
      ],
      "metadata": {
        "id": "cPwByi86ISIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Join operation on two different dataframes to get names of books on the basis of ISBN numbers\n",
        "\n",
        "books_rating_new = pd.merge(top_book_rating,books, how='inner', on = 'ISBN')\n",
        "books_rating_new = books_rating_new.iloc[:,0:3]\n",
        "books_rating_new.head(4)"
      ],
      "metadata": {
        "id": "AK9cIHYmNyN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comment_words = ''\n",
        "for val in books_rating_new['Book-Title']:\n",
        "     \n",
        "    # typecaste each val to string\n",
        "    val = str(val)\n",
        " \n",
        "    # split the value\n",
        "    tokens = val.split()\n",
        "     \n",
        "    # Converts each token into lowercase\n",
        "    for i in range(len(tokens)):\n",
        "        tokens[i] = tokens[i].lower()\n",
        "     \n",
        "    comment_words += \"_\".join(tokens)+\" \""
      ],
      "metadata": {
        "id": "KHInKyqhgai7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = set(STOPWORDS)\n",
        "\n",
        "wordcloud = WordCloud(width = 900, height = 450,\n",
        "                background_color ='white',\n",
        "                stopwords = stopwords,\n",
        "                min_font_size = 10).generate(comment_words)\n",
        " \n",
        "# plot the WordCloud image                      \n",
        "plt.figure(figsize = (8, 8), facecolor = None)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad = 0)\n",
        " \n",
        "plt.show()"
      ],
      "metadata": {
        "id": "j3S5j0f8imYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "books.head(3)"
      ],
      "metadata": {
        "id": "PYwBzCsah1no"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 11. What are the top author-publisher combinations?"
      ],
      "metadata": {
        "id": "lq-unDVNjHrg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def concat_str(st1,st2):\n",
        "  '''This function concatenet two strings'''\n",
        "  out = st1+' '+st2\n",
        "  return(out)"
      ],
      "metadata": {
        "id": "ljw_JJuAmNYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_author_publisher = pd.DataFrame(books.groupby(by=['Publisher','Book-Author'])['Book-Title'].count())\n",
        "top_author_publisher.reset_index(inplace=True)\n",
        "top_author_publisher.sort_values(by='Book-Title',ascending=False,inplace=True)\n",
        "top_author_publisher['Author_Publisher'] = top_author_publisher['Book-Author'].map(str) + '-' + top_author_publisher['Publisher'].map(str)\n",
        "top_author_publisher = top_author_publisher.head(15)\n",
        "top_author_publisher"
      ],
      "metadata": {
        "id": "sa4Nr-WEi4x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "barplot_fun(size=(14, 6),Data=top_author_publisher,X='Author_Publisher',Y='Book-Title',col='darkred')"
      ],
      "metadata": {
        "id": "HsYXH2uupR2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Let's create one final dataframe by combining above three different dataframes so that we could get, some more information about variables by establishing relationship in between them."
      ],
      "metadata": {
        "id": "eoCfoNtfrWG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.merge(ratings,books, how='inner', on = 'ISBN')\n",
        "df2 = pd.merge(df1,users, how='inner', on = 'User-ID')\n",
        "df = df2.copy()"
      ],
      "metadata": {
        "id": "iT-zv_fMqD8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(3)"
      ],
      "metadata": {
        "id": "Hxy-HsmZspPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create one column to distribute ratings in four categories bad,mediam,good,best."
      ],
      "metadata": {
        "id": "Bn51q6gZzEuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new = []\n",
        "for i in df['Book-Rating']:\n",
        "  if i<=3:\n",
        "    new.append('bad')\n",
        "  elif i<=6:\n",
        "    new.append('medium')\n",
        "  elif i<=8:\n",
        "    new.append('good')\n",
        "  else:\n",
        "    new.append('very_good')\n",
        "len(new)"
      ],
      "metadata": {
        "id": "uuAn8Vq2zCX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Book_Rating_Cat\"] = new"
      ],
      "metadata": {
        "id": "SM2g7jA70Zes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 12. Is there any relation between Age of users and ratings given by them??"
      ],
      "metadata": {
        "id": "VO7P-DBkuLPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Scatter(X,Y):\n",
        "  '''This function gives scatter plot of two contineous variables'''\n",
        "\n",
        "  x = df[X]\n",
        "  y = df[Y]\n",
        "  colors = df[Y]\n",
        "  sizes = df[Y]\n",
        "\n",
        "  plt.figure(figsize = (10, 6))\n",
        "  plt.ticklabel_format(style = 'plain')\n",
        "  plt.scatter(x, y, c = colors, s = sizes, alpha = 0.3, cmap = 'PRGn') #viridis\n",
        "  plt.colorbar()\n",
        "\n",
        "  # cor = df[x,y].corr()\n",
        "\n",
        "  # fig.text(0.9, 0.75, 'Correlation: '+str(cor),fontsize = 14,color ='black',ha ='right',va ='bottom',alpha = 1.0)\n",
        "  \n",
        "  plt.xlabel(X,size=14)\n",
        "  plt.ylabel(Y,size=14)\n",
        "  plt.title(X+\" vs \"+Y,size=16);"
      ],
      "metadata": {
        "id": "Ltan0Du5ucgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Scatter(X='Age',Y='Book-Rating')"
      ],
      "metadata": {
        "id": "YDttrcItutp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Peoples with more age are giving honest reviews as compared to the young peoples. We could see that thing from the density of our data points available in above graph.\n",
        "2. Either books are getting very good reviews, they are in the range of 8-10 or very bad reviews in the range of 1-4.\n",
        "3. Peoples with age more than 50 years are genrally giving good reviews."
      ],
      "metadata": {
        "id": "-9wn26nZwdmm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 13. Distribution of book-ratings pie-chart."
      ],
      "metadata": {
        "id": "mBWCmCG80qCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "books_ratng = value_count(Data=df,X=\"Book_Rating_Cat\")\n",
        "books_ratng"
      ],
      "metadata": {
        "id": "7rdzdZO11jHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Pie Chart\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "data = books_ratng['count']\n",
        "keys = books_ratng['Book_Rating_Cat']\n",
        "  \n",
        "# declaring exploding pie\n",
        "explode = [0.01,0.03,0.05,0.07]\n",
        "# define Seaborn color palette to use\n",
        "palette_color = sns.color_palette('Dark2')\n",
        "  \n",
        "# plotting data on chart\n",
        "plt.pie(data, labels=['bad','good','very_good','medium'], colors=palette_color,explode=explode,autopct='%.0f%%',textprops={'fontsize': 14})\n",
        "\n",
        "\n",
        "plt.legend(fontsize=14)\n",
        "\n",
        "plt.title(\"Book Rating Distribution\",size=16)\n",
        "# displaying chart\n",
        "plt.show() "
      ],
      "metadata": {
        "id": "rBQKpjNG0iaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 14. Is there any outliers present in the Age column??"
      ],
      "metadata": {
        "id": "v5Ii2AyedGa4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def outlier_per(df,x):\n",
        "\n",
        "  '''this function gives number of outliers present in the feature by using IQR method'''\n",
        "\n",
        "  q1 = df[x].quantile(0.25)\n",
        "  q3 = df[x].quantile(0.75)\n",
        "  iqr = q3-q1\n",
        "  lw = q1 - (1.5*iqr)\n",
        "  uw = q3 + (1.5*iqr)\n",
        "\n",
        "  cnt = 0\n",
        "  for i in df[x]:\n",
        "    if (i>uw):\n",
        "      cnt+=1\n",
        "    elif(i<lw):\n",
        "      cnt+=1\n",
        "  strng = f'{round(cnt*100/len(df),2)} % Outliers present in column {x} in number it is {cnt}'\n",
        "  strng2 = f'mean: {round(df[x].mean(),2)}'\n",
        "  strng3 = f'median: {round(df[x].median(),2)}'\n",
        "  strng4 = f'minimum: {round(df[x].min(),2)}'\n",
        "  strng5 = f'maximum: {round(df[x].max(),2)}'\n",
        "  strng6 = f'kurtosis: {round(df[x].kurt(),2)}'\n",
        "  strng7 = f'skewness: {round(df[x].skew(),2)}'\n",
        "  \n",
        "  return(strng,strng2,strng3,strng4,strng5,strng6,strng7)"
      ],
      "metadata": {
        "id": "YWo7EfXIeECO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "sns.boxplot(df['Age'])\n",
        "plt.title('Boxplot of Age',size=16)\n",
        "outlier_per(df=df,x='Age')"
      ],
      "metadata": {
        "id": "D0oBrUghfssh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 15. Correlation heatmap of contineous variables in dataframe"
      ],
      "metadata": {
        "id": "oPc3_HZvjd5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new = []\n",
        "for i in df['Year-Of-Publication']:\n",
        "  if i in ['DK Publishing Inc','Gallimard']:\n",
        "    new.append(0)\n",
        "  else:\n",
        "    new.append(int(i))\n",
        "df['Year-Of-Publication2'] = new\n",
        "df['Year-Of-Publication2'].dtype"
      ],
      "metadata": {
        "id": "Jmljqxo5ZdeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop('Year-Of-Publication',axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "SWclEryIckFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "df['country_encoded'] = le.fit_transform(df['country'])"
      ],
      "metadata": {
        "id": "8_iuyKUwj2n3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (15,7))\n",
        "colormap = plt.cm.plasma\n",
        "sns.heatmap(df.corr(),cmap = colormap,annot=True);\n",
        "sns.set(font_scale=1.1)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "egVunge-ctk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we could see in above heatmap, almost all the variables are independant of each other and they dont posess any kind of correlation in between them."
      ],
      "metadata": {
        "id": "EBiA9SkAlNUA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Peoples whom gave either good or very good rating to the books are older as compared to the peoples whom gave bad rating to the books."
      ],
      "metadata": {
        "id": "6bzDGgA-Lf29"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "H0: Mean age of peoples with good rating> Mean age of peoples with bad ratings\n",
        "\n",
        "H1: Mean age of peoples with good rating<= Mean age of peoples with bad ratings"
      ],
      "metadata": {
        "id": "3FOH6A2MMVGV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we have to compare mean of two distributions one for books with good rating and another for book with bad rating, we will be performing t-test in this case and we will use p-value approch to do testing for statistical significance. "
      ],
      "metadata": {
        "id": "DTOs42CiMkRd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, Let's form two different distributions one for good ratings and another for bad ratings by using above dataframe."
      ],
      "metadata": {
        "id": "5AcqCaviNSjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_bad = df[df['Book_Rating_Cat']=='bad']\n",
        "df_good = df[(df['Book_Rating_Cat']=='very_good')]\n",
        "dfb_age = df_bad['Age']\n",
        "dfg_age = df_good['Age']\n",
        "dfb_age = [x for x in dfb_age if x>=0]\n",
        "dfg_age = [x for x in dfg_age if x>=0]"
      ],
      "metadata": {
        "id": "tOoX_iNzMjaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import ttest_ind  \n",
        "    \n",
        "def t_test(x,y,alternative='both-sided'):\n",
        "        _, double_p = ttest_ind(x,y,equal_var = False)\n",
        "        if alternative == 'both-sided':\n",
        "            pval = double_p\n",
        "        elif alternative == 'greater':\n",
        "            if np.mean(x) > np.mean(y):\n",
        "                 pval = double_p/2.\n",
        "            else:\n",
        "                 pval = 1.0 - double_p/2.\n",
        "        elif alternative == 'less':\n",
        "            if np.mean(x) < np.mean(y):\n",
        "                 pval = double_p/2.\n",
        "            else:\n",
        "              pval = 1.0 - double_p/2.\n",
        "\n",
        "        op = 'Hence we are failed to reject null hypothesis (H0) for significane level 0.05'\n",
        "        if pval < 0.05:\n",
        "          op = 'Hence we are rejecting null hypothesis (H0) for significane level 0.05 '\n",
        "        return (f'P-Value: {pval}, {op}')"
      ],
      "metadata": {
        "id": "h2dzcb0VQQlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t_test(x=dfb_age,y=dfg_age,alternative='greater')"
      ],
      "metadata": {
        "id": "mWE5TVukQTov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hence from above test we could say that mean age of peoples with good rating is either less or equal to mean age of peoples with bad rating."
      ],
      "metadata": {
        "id": "w2i4XpybRH4I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Book_Rating_Cat_Encoded'] = le.fit_transform(df['Book_Rating_Cat'])\n",
        "df.head(3)"
      ],
      "metadata": {
        "id": "MJ7NBCadUUAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ratings of peoples are changing with change in there location. That means geographical location of a person is also affecting the ratings given by them."
      ],
      "metadata": {
        "id": "TGAecLy-UsWW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "H0: There is relationship between country column and ratings.\n",
        "\n",
        "H1: There is no relationship between country column and ratings."
      ],
      "metadata": {
        "id": "ZpEnIyZfVBko"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we are analysing two categorical variables, here We are going to use chi-squre test of independance."
      ],
      "metadata": {
        "id": "phhzdJFVVU9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new = pd.crosstab(df['country_encoded'],df['Book_Rating_Cat_Encoded'])\n",
        "new = pd.DataFrame(new)\n",
        "new.reset_index(inplace=True)\n",
        "new['Total'] = new[0] + new[1] + new[2] + new[3]\n",
        "new = new.iloc[1:,:]\n",
        "new.set_index('country_encoded',inplace=True)\n",
        "new.loc['Total'] = new.iloc[:, :].sum()\n",
        "new"
      ],
      "metadata": {
        "id": "nBL5R_KIVllW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# significance level\n",
        "alpha = 0.05\n",
        "\n",
        "# Calcualtion of Chisquare\n",
        "chi_square = 0\n",
        "rows = new.index.unique()\n",
        "columns = new.columns.unique()\n",
        "for i in columns:\n",
        "    for j in rows:\n",
        "        O = new[i][j]\n",
        "        E = new[i]['Total'] * new['Total'][j] / new['Total']['Total']\n",
        "        chi_square += (O-E)**2/E"
      ],
      "metadata": {
        "id": "yqAniTDpX4LZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The p-value approach\n",
        "print(\"Approach 1: The p-value approach to hypothesis testing in the decision rule\")\n",
        "p_value = 1 - stats.chi2.cdf(chi_square, (len(rows)-1)*(len(columns)-1))\n",
        "conclusion = \"Failed to reject the null hypothesis.\"\n",
        "if p_value <= alpha:\n",
        "    conclusion = \"Null Hypothesis is rejected.\"\n",
        "        \n",
        "print(\"chisquare-score is:\", chi_square, \" and p value is:\", p_value)\n",
        "print(conclusion)"
      ],
      "metadata": {
        "id": "GMZR2ZkqX8o1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hence from conclusion of above test we could say that there is no relationship in between geographical location and ratings given by peoples."
      ],
      "metadata": {
        "id": "Wu5agD79YqNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(3)"
      ],
      "metadata": {
        "id": "VS5d-1aaqaDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time span in which books were published affecting ratings given by users."
      ],
      "metadata": {
        "id": "_Is6vuT-sHX9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "H0: There is relationship between ratings column and time span.\n",
        "\n",
        "H1: There is no relationship between ratings column and time span."
      ],
      "metadata": {
        "id": "xrurIHARse8X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we are analysing two categorical variables, here We are going to use chi-squre test of independance."
      ],
      "metadata": {
        "id": "g3ebS9gbsv-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new = []\n",
        "for i in df['Year-Of-Publication2']:\n",
        "  if i <= 1990:\n",
        "    new.append(\"before_90's\")\n",
        "  elif i <= 1995:\n",
        "    new.append(\"90_95\")\n",
        "  elif i <= 2000:\n",
        "    new.append(\"95_2000\")\n",
        "  else:\n",
        "    new.append(\"after_2000\")\n",
        "  \n",
        "df[\"time_span\"] = new"
      ],
      "metadata": {
        "id": "PDrDcg09sz9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new = pd.crosstab(df['time_span'],df['Book_Rating_Cat_Encoded'])\n",
        "new = pd.DataFrame(new)\n",
        "new.reset_index(inplace=True)\n",
        "new['Total'] = new[0] + new[1] + new[2] + new[3]\n",
        "new = new.iloc[1:,:]\n",
        "new.set_index('time_span',inplace=True)\n",
        "new.loc['Total'] = new.iloc[:, :].sum()\n",
        "new"
      ],
      "metadata": {
        "id": "2ycMj_zXupmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# significance level\n",
        "alpha = 0.05\n",
        "\n",
        "# Calcualtion of Chisquare\n",
        "chi_square = 0\n",
        "rows = new.index.unique()\n",
        "columns = new.columns.unique()\n",
        "for i in columns:\n",
        "    for j in rows:\n",
        "        O = new[i][j]\n",
        "        E = new[i]['Total'] * new['Total'][j] / new['Total']['Total']\n",
        "        chi_square += (O-E)**2/E"
      ],
      "metadata": {
        "id": "6sPq0JN8vBpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The p-value approach\n",
        "print(\"Approach 1: The p-value approach to hypothesis testing in the decision rule\")\n",
        "p_value = 1 - stats.chi2.cdf(chi_square, (len(rows)-1)*(len(columns)-1))\n",
        "conclusion = \"Failed to reject the null hypothesis.\"\n",
        "if p_value <= alpha:\n",
        "    conclusion = \"Null Hypothesis is rejected.\"\n",
        "        \n",
        "print(\"chisquare-score is:\", chi_square, \" and p value is:\", p_value)\n",
        "print(conclusion)"
      ],
      "metadata": {
        "id": "t8erT0kdvGik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hence from conclusion of above test we could say that there is no relationship in between time_span of book published and ratings given by peoples."
      ],
      "metadata": {
        "id": "IFG4TcvXvQCt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "\n",
        "'''We have around 35% null values in Age column so we are going to replace them with mean \n",
        "value of Age coulmn as we have seen above, by doing hypothesis testing that ratings are not getting \n",
        "affected due to age, so we can replace them with mean of that column''' #37.39764848314286\n",
        "\n",
        "df['Age'] = df['Age'].fillna(37)"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''In book-author column and publisher column there are only 3 null values present so we are going\n",
        "to drop them directly as they are not considerable in number in such a huge dataset with more than 10,00,000 \n",
        "records'''\n",
        "\n",
        "df.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "9DolYiETh0XV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' We have removed all the missing values present in our dataset by using different methods\n",
        "for different columns which are mentioned above'''\n",
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "vyInc94dhiJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "\n",
        "''' As we have only one contineous column which is age and we have already checked \n",
        "around 6.70% outliers are present in the age column so we are going to replace values \n",
        "which are greater than 95 by using upper whisker value and values lower than 10 \n",
        "by using lw'''\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def outlier_detection(df,x):\n",
        "\n",
        "  '''this function gives lower whicker and upper whisker of the feature by using IQR method'''\n",
        "\n",
        "  q1 = df[x].quantile(0.25)\n",
        "  q3 = df[x].quantile(0.75)\n",
        "  iqr = q3-q1\n",
        "  lw = q1 - (1.5*iqr)\n",
        "  uw = q3 + (1.5*iqr)\n",
        "  return(lw,uw)"
      ],
      "metadata": {
        "id": "yWpd7KFIj1s2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def outlier_replace(df,x):\n",
        "\n",
        "  '''this function replaces outliers lower than lower whisker with lower whicker and outliers higher than upper whisker with upper whicker'''\n",
        "\n",
        "  lw,uw = outlier_detection(df,x)\n",
        "  lstn = []\n",
        "  for i in df[x]:\n",
        "    if (i<10):\n",
        "      lstn.append(uw)\n",
        "    elif (i>95):\n",
        "      lstn.append(uw)\n",
        "    else:\n",
        "      lstn.append(i)\n",
        "  df[x+'_out_replace'] = lstn"
      ],
      "metadata": {
        "id": "6FfMygoojwSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outlier_replace(df=df,x='Age')"
      ],
      "metadata": {
        "id": "fxTu_odQkQoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def boxplot_out(df,X,Y):\n",
        "\n",
        "    '''this function gives boxplot of variables with and without outliers'''\n",
        "\n",
        "    ## detecting outliers and calculating there %\n",
        "\n",
        "    strng0 = f'Boxplot of {X}'\n",
        "    strng = outlier_per(df,X)[0]\n",
        "    strng2 = outlier_per(df,X)[1]\n",
        "    strng3 = outlier_per(df,X)[2]\n",
        "    strng4 = outlier_per(df,X)[3]\n",
        "    strng5 = outlier_per(df,X)[4]\n",
        "    strng6 = outlier_per(df,X)[5]\n",
        "    strng7 = outlier_per(df,X)[6]\n",
        "    \n",
        "\n",
        "    ## plotting figure\n",
        "    fig, axes = plt.subplots(1,2, figsize=(22,6))\n",
        "    sns.boxplot(ax=axes[0], data=df,x=X)\n",
        "\n",
        "    ##labels\n",
        "\n",
        "    fig.text(0.35, 0.9,strng0,fontsize = 16,color ='black',ha ='right',va ='bottom',alpha = 1.0)\n",
        "    fig.text(0.45, 0.75,strng,fontsize = 12,color ='black',ha ='right',va ='bottom',alpha = 1.0)\n",
        "    fig.text(0.45, 0.70,strng2,fontsize = 12,color ='black',ha ='right',va ='bottom',alpha = 1.0)\n",
        "    fig.text(0.45, 0.65,strng3,fontsize = 12,color ='black',ha ='right',va ='bottom',alpha = 1.0)\n",
        "    fig.text(0.45, 0.60,strng4,fontsize = 12,color ='black',ha ='right',va ='bottom',alpha = 1.0)\n",
        "    fig.text(0.45, 0.55,strng5,fontsize = 12,color ='black',ha ='right',va ='bottom',alpha = 1.0)\n",
        "    fig.text(0.45, 0.50,strng6,fontsize = 12,color ='black',ha ='right',va ='bottom',alpha = 1.0)\n",
        "    fig.text(0.45, 0.45,strng7,fontsize = 12,color ='black',ha ='right',va ='bottom',alpha = 1.0)\n",
        "\n",
        "    strng0 = f'Boxplot of {Y}'\n",
        "    strng = outlier_per(df,Y)[0]\n",
        "    strng2 = outlier_per(df,Y)[1]\n",
        "    strng3 = outlier_per(df,Y)[2]\n",
        "    strng4 = outlier_per(df,Y)[3]\n",
        "    strng5 = outlier_per(df,Y)[4]\n",
        "    strng6 = outlier_per(df,Y)[5]\n",
        "    strng7 = outlier_per(df,Y)[6]\n",
        "    \n",
        "\n",
        "    ## plotting figure\n",
        "    # fig, axes = plt.subplots(1,2, figsize=(22,6))\n",
        "    sns.boxplot(ax=axes[1], data=df,x=Y)\n",
        "\n",
        "    ##labels\n",
        "    # plt.title(f'Boxplot of {Y}',size=14)\n",
        "\n",
        "    fig.text(0.80, 0.9,strng0,fontsize = 16,color ='black',ha ='right',va ='bottom',alpha = 1.0)\n",
        "    fig.text(0.85, 0.75,strng,fontsize = 12,color ='black',ha ='right',va ='bottom',alpha = 1.0)\n",
        "    fig.text(0.85, 0.70,strng2,fontsize = 12,color ='black',ha ='right',va ='bottom',alpha = 1.0)\n",
        "    fig.text(0.85, 0.65,strng3,fontsize = 12,color ='black',ha ='right',va ='bottom',alpha = 1.0)\n",
        "    fig.text(0.85, 0.60,strng4,fontsize = 12,color ='black',ha ='right',va ='bottom',alpha = 1.0)\n",
        "    fig.text(0.85, 0.55,strng5,fontsize = 12,color ='black',ha ='right',va ='bottom',alpha = 1.0)\n",
        "    fig.text(0.85, 0.50,strng6,fontsize = 12,color ='black',ha ='right',va ='bottom',alpha = 1.0)\n",
        "    fig.text(0.85, 0.45,strng7,fontsize = 12,color ='black',ha ='right',va ='bottom',alpha = 1.0)\n",
        "    \n",
        "    # show plot\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "vH8DwLXOkUsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "boxplot_out(df=df,X='Age',Y='Age_out_replace')"
      ],
      "metadata": {
        "id": "usqoIWcYkmTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "df.drop(['Location','Age','Book_Rating_Cat','time_span','country'],axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have already encoded columns in our dataset by using, various methods which are explained above."
      ],
      "metadata": {
        "id": "smFRZwrWuZFX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Feature Manipulation & Selection**"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features \n",
        "''' We have already did enough data manipulation at the time of doing exploratory\n",
        "data analysis now there is no any need to do data manipulation as for further\n",
        "processes we don't need those futures we are going to deal with only books's ISBN number, \n",
        "User Id and ratings given by those users.'''"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting \n",
        "dffinal = df.copy()\n",
        "df = df[['User-ID','ISBN','Book-Rating']]\n",
        "df.shape"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dffinal = dffinal.iloc[0:50000,:]\n",
        "df = df.iloc[0:50000,:]\n",
        "print(df.shape)\n",
        "print(dffinal.shape)"
      ],
      "metadata": {
        "id": "EUCmgXHFRqPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dffinal.head(3)"
      ],
      "metadata": {
        "id": "_KdfQFmAUWhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.rename(columns={'ISBN':'contentId'},inplace=True)\n",
        "df.head(3)"
      ],
      "metadata": {
        "id": "AC4NPsdlUack"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5. Approach to build recommendation engine**"
      ],
      "metadata": {
        "id": "5s9WisBUBlma"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why collborative filtering, not content based filtering ??** \n",
        "\n",
        "By doing hypothesis testing, We have cross checked statistical significance of all those insights that we were getting through various kind of graphs and after testing those hypothesis we have reached out to a conclusion that ratings doesn't have much dependancy on the various features mentioned above, instead they are varying with respect to users to users. Hence content-based approach could not be a better choice in this case, that's why we have decided to go with collaborative filtering in this case to build a recommendation engine."
      ],
      "metadata": {
        "id": "TW9PEGryHKL9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Cold Start Problem**"
      ],
      "metadata": {
        "id": "IHNNbMriBtiC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5c92aa80-2926-44db-b358-c4c32de806c4",
        "_uuid": "91100a395fdf4fb20df02c8d248072457c980b5d",
        "id": "zZeu_QOWOlwF"
      },
      "source": [
        "##### Recommender systems have a problem known as user cold-start, in which it is hard to provide personalized recommendations for users with none or a very few number of consumed items, due to the lack of information to model their preferences.  \n",
        "\n",
        "##### For this reason, we are keeping in the dataset only users with at least 5 interactions."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "users_interactions_count_df = df.groupby(['User-ID', 'contentId']).size().groupby('User-ID').size()\n",
        "print('# of users: %d' % len(users_interactions_count_df))\n",
        "\n",
        "users_with_enough_interactions_df = users_interactions_count_df[users_interactions_count_df >= 5].reset_index()[['User-ID']]\n",
        "print('# of users with at least 5 interactions: %d' % len(users_with_enough_interactions_df))"
      ],
      "metadata": {
        "id": "e-qKs5PrB7TG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('# of interactions: %d' % len(df))\n",
        "interactions_from_selected_users_df = df.merge(users_with_enough_interactions_df, \n",
        "               how = 'right',\n",
        "               left_on = 'User-ID',\n",
        "               right_on = 'User-ID')\n",
        "print('# of interactions from users with at least 5 interactions: %d' % len(interactions_from_selected_users_df))"
      ],
      "metadata": {
        "id": "6-Im8vPeD1Ll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interactions_full_df = interactions_from_selected_users_df.copy()\n",
        "interactions_full_df.rename(columns={'User-ID':'personId'},inplace=True)\n",
        "interactions_full_df.head(3)"
      ],
      "metadata": {
        "id": "szt3URbJELvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "\n",
        "interactions_train_df, interactions_test_df = train_test_split(interactions_full_df,\n",
        "                                   stratify=interactions_full_df['personId'], \n",
        "                                   test_size=0.80,\n",
        "                                   random_state=42)\n",
        "\n",
        "print('# interactions on Train set: %d' % len(interactions_train_df))\n",
        "print('# interactions on Test set: %d' % len(interactions_test_df))"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interactions_test_df.head()"
      ],
      "metadata": {
        "id": "DcwSREf7FpWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6. Model Building**"
      ],
      "metadata": {
        "id": "f4VCA9qgBHhq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why Model-Based approach instead of memory based approach??**\n",
        "\n",
        "As we have to many users in our datset and hence memory-based approach could not scale better in our case, so that's why we have used model-based approach to solve our problem."
      ],
      "metadata": {
        "id": "7cnDPsT4H_6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# interactions_full_df.to_csv(\"book_recommendation.csv\")"
      ],
      "metadata": {
        "id": "PDQN6MrebSRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a sparse pivot table with users in rows and items in columns\n",
        "\n",
        "users_items_pivot_matrix_df = interactions_train_df.pivot(index='personId',columns='contentId',values='Book-Rating').fillna(0)\n",
        "users_items_pivot_matrix_df.head()"
      ],
      "metadata": {
        "id": "jYUca4KyFywk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users_items_pivot_matrix = users_items_pivot_matrix_df.values\n",
        "users_items_pivot_matrix[:10]"
      ],
      "metadata": {
        "id": "ops_fEDbW4Tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users_ids = list(users_items_pivot_matrix_df.index)\n",
        "users_ids[:10]"
      ],
      "metadata": {
        "id": "-a_AuYLc8G5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The number of factors to factor the user-item matrix.\n",
        "NUMBER_OF_FACTORS_MF = 15\n",
        "\n",
        "#Performs matrix factorization of the original user item matrix\n",
        "U, sigma, Vt = svds(users_items_pivot_matrix, k = NUMBER_OF_FACTORS_MF)"
      ],
      "metadata": {
        "id": "miB8iRrf8NxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users_items_pivot_matrix.shape"
      ],
      "metadata": {
        "id": "ILv_bplK8Zc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "U.shape"
      ],
      "metadata": {
        "id": "6Ta-DUlA8ajy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sigma = np.diag(sigma)\n",
        "sigma.shape"
      ],
      "metadata": {
        "id": "EHm8ZlGU8fmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Vt.shape"
      ],
      "metadata": {
        "id": "9GZDRvQa8h6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5cf974d4-ca0c-487a-8561-7952be1d2f16",
        "_uuid": "de8f01beee0d5fad5e4cfd614b7c86c36b827958",
        "id": "PWUv4X7rOlwK"
      },
      "source": [
        "##### After the factorization, we try to to reconstruct the original matrix by multiplying its factors. The resulting matrix is not sparse any more. It was generated predictions for items the user have not yet interaction, which we will exploit for recommendations."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_user_predicted_ratings = np.dot(np.dot(U, sigma), Vt) \n",
        "all_user_predicted_ratings"
      ],
      "metadata": {
        "id": "XqS9VesN8y4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_user_predicted_ratings.shape"
      ],
      "metadata": {
        "id": "_8xMIbin83Vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting the reconstructed matrix back to a Pandas dataframe\n",
        "cf_preds_df = pd.DataFrame(all_user_predicted_ratings, columns = users_items_pivot_matrix_df.columns, index=users_ids).transpose()\n",
        "# cf_preds_df = cf_preds_df.rename_axis('contentID').reset_index()\n",
        "cf_preds_df.head()"
      ],
      "metadata": {
        "id": "plbJf4K-8_WZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(cf_preds_df.columns)"
      ],
      "metadata": {
        "id": "3lB5LObY9DBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dffinal.rename(columns={'User-ID':'personId','ISBN':'contentId'},inplace=True)\n",
        "dffinal.head(3)"
      ],
      "metadata": {
        "id": "ZrY4xOjAtfKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CFRecommender:\n",
        "    \n",
        "    MODEL_NAME = 'Collaborative Filtering'\n",
        "    \n",
        "    def __init__(self, cf_predictions_df, items_df=None):\n",
        "        self.cf_predictions_df = cf_predictions_df\n",
        "        self.items_df = items_df\n",
        "        \n",
        "    def get_model_name(self):\n",
        "        return self.MODEL_NAME\n",
        "        \n",
        "    def recommend_items(self, personId, items_to_ignore=[], topn=10, verbose=False):\n",
        "        # Get and sort the user's predictions\n",
        "        sorted_user_predictions = self.cf_predictions_df[personId].sort_values(ascending=False).reset_index().rename(columns={personId: 'recStrength'})\n",
        "\n",
        "        # Recommend the highest predicted rating content that the user hasn't seen yet.\n",
        "        recommendations_df = sorted_user_predictions[~sorted_user_predictions['contentId'].isin(items_to_ignore)].sort_values('recStrength', ascending = False).head(topn)\n",
        "\n",
        "        if verbose:\n",
        "            if self.items_df is None:\n",
        "                raise Exception('\"items_df\" is required in verbose mode')\n",
        "\n",
        "            recommendations_df = recommendations_df.merge(self.items_df, how = 'left', \n",
        "                                                          left_on = 'contentId', \n",
        "                                                          right_on = 'contentId')[['recStrength','contentId','Book-Title', 'Publisher', 'Book-Author']]\n",
        "\n",
        "\n",
        "        return recommendations_df\n",
        "\n",
        "cf_recommender_model = CFRecommender(cf_preds_df,dffinal) "
      ],
      "metadata": {
        "id": "UYnw_LvL9IFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cf_recommender_model.recommend_items(personId=2313,topn=5)"
      ],
      "metadata": {
        "id": "E5qQTOdvPf_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def books_recommended(personId):\n",
        "  new = cf_recommender_model.recommend_items(personId=2313,topn=5)\n",
        "  new = new[['contentId']]"
      ],
      "metadata": {
        "id": "4dEyMdXPx8nr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7db22656-f1b2-4264-b74d-92ef7e03168e",
        "_uuid": "999dac17031a334be5a2245086e9c4655c5e8324",
        "id": "tKFRgbLkOlwL"
      },
      "source": [
        "# **7. Evaluation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "be788cfc-2733-4184-8dfb-24d789e3b7da",
        "_uuid": "9a1ce40c5b2b7f89c2e34a8fce1adbeb0cfabf46",
        "id": "yYvdSbpzOlwM"
      },
      "source": [
        "##### In Recommender Systems, there are a set metrics commonly used for evaluation. We choose to work with **Top-N accuracy metrics**, which evaluates the accuracy of the top recommendations provided to a user, comparing to the items the user has actually interacted in test set.\n",
        "\n",
        "##### This evaluation method works as follows:\n",
        "\n",
        "* ##### For each user\n",
        "    * ##### For each item the user has interacted in test set\n",
        "        * ##### Sample 10 other items the user has never interacted.   \n",
        "        * ##### Ask the recommender model to produce a ranked list of recommended items, from a set composed of one interacted item and the 10 non-interacted items\n",
        "        * ##### Compute the Top-N accuracy metrics for this user and interacted item from the recommendations ranked list\n",
        "* ##### Aggregate the global Top-N accuracy metrics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Indexing by personId to speed up the searches during evaluation\n",
        "interactions_full_indexed_df = interactions_full_df.set_index('personId')\n",
        "interactions_train_indexed_df = interactions_train_df.set_index('personId')\n",
        "interactions_test_indexed_df = interactions_test_df.set_index('personId')"
      ],
      "metadata": {
        "id": "Ddc2ZDT6B17i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a6f711db-3848-42de-9cb1-8adbe9fffbbd",
        "_uuid": "cb9da9e9269f20d347c9a7d0320da02f5b5d9cda",
        "id": "Dyr41y81OlwM"
      },
      "source": [
        "##### The Top-N accuracy metric choosen was **Recall@N** which evaluates whether the interacted item is among the top N items (hit) in the ranked list of 10 recommendations for a user."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_items_interacted(person_id, interactions_df):\n",
        "    interacted_items = interactions_df.loc[person_id]['contentId']\n",
        "    return set(interacted_items if type(interacted_items) == pd.Series else [interacted_items])"
      ],
      "metadata": {
        "id": "LI2OZ76GCWlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "0e0639e7-4041-473d-a090-cd0087ce92c3",
        "_uuid": "c9612b159a8d626fe986586230b829ce2e93aff7",
        "id": "fNI6CqG9OlwM"
      },
      "source": [
        "#Top-N accuracy metrics consts\n",
        "EVAL_RANDOM_SAMPLE_NON_INTERACTED_ITEMS = 10\n",
        "\n",
        "class ModelEvaluator:\n",
        "\n",
        "    # Function for getting the set of items which a user has not interacted with\n",
        "    def get_not_interacted_items_sample(self, person_id, sample_size, seed=42):\n",
        "        interacted_items = get_items_interacted(person_id, interactions_full_indexed_df)\n",
        "        all_items = set(dffinal['contentId'])\n",
        "        non_interacted_items = all_items - interacted_items\n",
        "\n",
        "        random.seed(seed)\n",
        "        non_interacted_items_sample = random.sample(non_interacted_items, sample_size)\n",
        "        return set(non_interacted_items_sample)\n",
        "\n",
        "    # Function to verify whether a particular item_id was present in the set of top N recommended items\n",
        "    def _verify_hit_top_n(self, item_id, recommended_items, topn):        \n",
        "            try:\n",
        "                index = next(i for i, c in enumerate(recommended_items) if c == item_id)\n",
        "            except:\n",
        "                index = -1\n",
        "            hit = int(index in range(0, topn))\n",
        "            return hit, index\n",
        "    \n",
        "    # Function to evaluate the performance of model for each user\n",
        "    def evaluate_model_for_user(self, model, person_id):\n",
        "        \n",
        "        # Getting the items in test set\n",
        "        interacted_values_testset = interactions_test_indexed_df.loc[person_id]\n",
        "        \n",
        "        if type(interacted_values_testset['contentId']) == pd.Series:\n",
        "            person_interacted_items_testset = set(interacted_values_testset['contentId'])\n",
        "        else:\n",
        "            person_interacted_items_testset = set([int(interacted_values_testset['contentId'])])\n",
        "            \n",
        "        interacted_items_count_testset = len(person_interacted_items_testset) \n",
        "\n",
        "        # Getting a ranked recommendation list from the model for a given user\n",
        "        person_recs_df = model.recommend_items(person_id, items_to_ignore=get_items_interacted(person_id, interactions_train_indexed_df),topn=10000000000)\n",
        "\n",
        "        hits_at_5_count = 0\n",
        "        hits_at_10_count = 0\n",
        "        \n",
        "        # For each item the user has interacted in test set\n",
        "        for item_id in person_interacted_items_testset:\n",
        "            \n",
        "            # Getting a random sample of 100 items the user has not interacted with\n",
        "            non_interacted_items_sample = self.get_not_interacted_items_sample(person_id, sample_size=EVAL_RANDOM_SAMPLE_NON_INTERACTED_ITEMS, seed=42)\n",
        "\n",
        "            # Combining the current interacted item with the 100 random items\n",
        "            items_to_filter_recs = non_interacted_items_sample.union(set([item_id]))\n",
        "\n",
        "            # Filtering only recommendations that are either the interacted item or from a random sample of 100 non-interacted items\n",
        "            valid_recs_df = person_recs_df[person_recs_df['contentId'].isin(items_to_filter_recs)]                    \n",
        "            valid_recs = valid_recs_df['contentId'].values\n",
        "            \n",
        "            # Verifying if the current interacted item is among the Top-N recommended items\n",
        "            hit_at_5, index_at_5 = self._verify_hit_top_n(item_id, valid_recs, 5)\n",
        "            hits_at_5_count += hit_at_5\n",
        "            hit_at_10, index_at_10 = self._verify_hit_top_n(item_id, valid_recs, 10)\n",
        "            hits_at_10_count += hit_at_10\n",
        "\n",
        "        # Recall is the rate of the interacted items that are ranked among the Top-N recommended items\n",
        "        recall_at_5 = hits_at_5_count / float(interacted_items_count_testset)\n",
        "        recall_at_10 = hits_at_10_count / float(interacted_items_count_testset)\n",
        "\n",
        "        person_metrics = {'hits@5_count':hits_at_5_count, \n",
        "                          'hits@10_count':hits_at_10_count, \n",
        "                          'interacted_count': interacted_items_count_testset,\n",
        "                          'recall@5': recall_at_5,\n",
        "                          'recall@10': recall_at_10}\n",
        "        return person_metrics\n",
        "\n",
        "    \n",
        "    # Function to evaluate the performance of model at overall level\n",
        "    def evaluate_model(self, model):\n",
        "        \n",
        "        people_metrics = []\n",
        "        \n",
        "        for idx, person_id in enumerate(list(interactions_test_indexed_df.index.unique().values)):    \n",
        "            person_metrics = self.evaluate_model_for_user(model, person_id)  \n",
        "            person_metrics['_person_id'] = person_id\n",
        "            people_metrics.append(person_metrics)\n",
        "            \n",
        "        print('%d users processed' % idx)\n",
        "\n",
        "        detailed_results_df = pd.DataFrame(people_metrics).sort_values('interacted_count', ascending=False)\n",
        "        \n",
        "        global_recall_at_5 = detailed_results_df['hits@5_count'].sum() / float(detailed_results_df['interacted_count'].sum())\n",
        "        global_recall_at_10 = detailed_results_df['hits@10_count'].sum() / float(detailed_results_df['interacted_count'].sum())\n",
        "        \n",
        "        global_metrics = {'modelName': model.get_model_name(),\n",
        "                          'recall@5': global_recall_at_5,\n",
        "                          'recall@10': global_recall_at_10}    \n",
        "        return global_metrics, detailed_results_df\n",
        "    \n",
        "model_evaluator = ModelEvaluator()    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_evaluator.get_not_interacted_items_sample(person_id=2313,sample_size=100)"
      ],
      "metadata": {
        "id": "xQ2-Lv5cZ2oM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Evaluating Collaborative Filtering (SVD Matrix Factorization) model...')\n",
        "cf_global_metrics,cf_detailed_results_df = model_evaluator.evaluate_model(cf_recommender_model)\n",
        "\n",
        "print('\\nGlobal metrics:\\n%s' % cf_global_metrics)\n",
        "cf_detailed_results_df.head(10)"
      ],
      "metadata": {
        "id": "Z9y9V71Mmn71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. We have performed exploratory data analysis on all three dataset which contains information about books,users and ratings given by those users to various books.\n",
        "2. Due to EDA we were able to find out relationship in between various users and books they are using, detailed descriptions about users geographical identities, relationship in between various features such as age and ratings, country and ratings etc.\n",
        "3. By doing hypothesis testing, We have cross checked statistical significance of all those insights that we were getting through various kind of graphs and after testing those hypothesis we have reached out to a conclusion that ratings doesn't have much dependancy on the various features mentioned above, instead they are varying with respect to users to users. Hence content-based approach could not be a better choice in this case, that's why we have decided to go with collaborative filtering in this case to build a recommendation engine.\n",
        "4. As we have to many users in our datset and hence memory-based approach could not scale better in our case, so that why we have used model-based approach to solve our problem.\n",
        "5. By using Singular Value Decomposition (SVD) model which is one of the  Latent Factor Model we have built our recommendation and we are able to recommend books to new users, with around 20% recall rate on such a huge dataset."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! Let's recommend books for our users !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}