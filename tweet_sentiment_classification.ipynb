{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00730149-9b1a-4f1a-90ff-d247afd2eb8e",
   "metadata": {},
   "source": [
    "# Tweet Sentiment Classification\n",
    "\n",
    "Build and train models to classify tweet sentiment as positive or negative using `Logistic Regression` and `Naïve Bayes` classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "6f0c5fd5-3029-4c7e-840b-110eed0904d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import twitter_samples\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d52232-5944-47a2-98a9-868edc6a9aa0",
   "metadata": {},
   "source": [
    "## Download and explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "33c725ef-3615-4032-9d53-1103262e3e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download twitter samples and unzip \n",
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "0328a375-f28e-4a57-b0b0-0398d458fda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load postive and negative tweets from JSON to list\n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "7047374f-c557-4279-893e-f418fb77bb21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 5000)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(positive_tweets), len(negative_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "ce040545-6b0e-41a5-a666-e586b2b708ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Tweets...\n",
      "-------------------\n",
      "Tweet 1:  #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "Tweet 2:  @Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!\n",
      "Tweet 3:  @DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!\n",
      "Tweet 4:  @97sides CONGRATS :)\n",
      "Tweet 5:  yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days\n",
      "\n",
      "Negative Tweets...\n",
      "-------------------\n",
      "Tweet 1:  hopeless for tmr :(\n",
      "Tweet 2:  Everything in the kids section of IKEA is so cute. Shame I'm nearly 19 in 2 months :(\n",
      "Tweet 3:  @Hegelbon That heart sliding into the waste basket. :(\n",
      "Tweet 4:  “@ketchBurning: I hate Japanese call him \"bani\" :( :(”\n",
      "\n",
      "Me too\n",
      "Tweet 5:  Dang starting next week I have \"work\" :(\n"
     ]
    }
   ],
   "source": [
    "# View few samples tweets\n",
    "print('Positive Tweets...\\n-------------------')\n",
    "for i, tweet in enumerate(positive_tweets[:5]): print(f'Tweet {i+1}: ', tweet)\n",
    "print('\\nNegative Tweets...\\n-------------------')\n",
    "for i, tweet in enumerate(negative_tweets[:5]): print(f'Tweet {i+1}: ', tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e71fbd-c2e7-431d-87ff-06061b7220f2",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "9fab1a44-e3bc-4876-96aa-3f59b99c6ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80% train (4000 tweets) and 20% test (1000 tweets)\n",
    "train_pos_tweets = positive_tweets[:4000]\n",
    "test_pos_tweets = positive_tweets[4000:]\n",
    "train_neg_tweets = negative_tweets[:4000]\n",
    "test_neg_tweets = negative_tweets[4000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fc5ce6-da91-4745-b285-a4304cee89e5",
   "metadata": {},
   "source": [
    "## Preprocess data\n",
    "Tweets needs to be cleaned and encoded to make them ready for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "50ae97b2-ceb7-4267-ac14-929ae0031d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load required meta data for utils routines\n",
    "utils.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "64bc8192-f0a8-41d5-81e2-1c7d2c294d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all training tweets, clean and tokenize them\n",
    "train_pos_tweets = [ utils.clean_and_tokenize_tweet(tweet) \n",
    "                     for tweet in train_pos_tweets\n",
    "                   ]\n",
    "train_neg_tweets = [ utils.clean_and_tokenize_tweet(tweet) \n",
    "                     for tweet in train_neg_tweets\n",
    "                   ]\n",
    "\n",
    "# Loop through all test tweets, clean and tokenize them\n",
    "test_pos_tweets = [ utils.clean_and_tokenize_tweet(tweet) \n",
    "                     for tweet in test_pos_tweets\n",
    "                   ]\n",
    "test_neg_tweets = [ utils.clean_and_tokenize_tweet(tweet) \n",
    "                     for tweet in test_neg_tweets\n",
    "                   ]\n",
    "# Random shuffle\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(train_pos_tweets)\n",
    "np.random.shuffle(train_neg_tweets)\n",
    "np.random.shuffle(test_pos_tweets)\n",
    "np.random.shuffle(test_neg_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fb36f7-31c8-4d34-8bf4-fd274b14d711",
   "metadata": {},
   "source": [
    "### Encode tweets and extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "bdd9151b-6bc0-4ab0-97b4-a2de197707c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Build frequency dictionary using training set...\n",
    "pos_freq_dict = utils.build_freq_dict(train_pos_tweets) \n",
    "neg_freq_dict = utils.build_freq_dict(train_neg_tweets)\n",
    "\n",
    "# Store frequencies for latter use\n",
    "utils.meta_data['pos_freq_dict'] = pos_freq_dict\n",
    "utils.meta_data['neg_freq_dict'] = neg_freq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "affcb851-69eb-4ea6-a9bd-da5b227c6eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Encode each tweet in corpus as features [bias (always 1), \n",
    "#    sum-of-positive-freq, sum-of-positive-freq] then build a matrix \n",
    "#    containing all encoded tweets and a vector containing corresponding \n",
    "#    labels...\n",
    "\n",
    "# process training positive tweets\n",
    "X1, y1 = utils.build_feature_matrix(train_pos_tweets, 1, \n",
    "                                    pos_freq_dict, neg_freq_dict)\n",
    "# process training negative tweets\n",
    "X2, y2 = utils.build_feature_matrix(train_neg_tweets, 0, \n",
    "                              pos_freq_dict, neg_freq_dict)\n",
    "# Combine both training matrices to make a single training dataset\n",
    "X_train = np.vstack((X1, X2))\n",
    "y_train = np.vstack((y1, y2)).squeeze()\n",
    "\n",
    "# process test positive tweets\n",
    "X1, y1 = utils.build_feature_matrix(test_pos_tweets, 1, \n",
    "                                    pos_freq_dict, neg_freq_dict)\n",
    "# process test negative tweets\n",
    "X2, y2 = utils.build_feature_matrix(test_neg_tweets, 0, \n",
    "                              pos_freq_dict, neg_freq_dict)\n",
    "# Combine both test matrices to make a single test dataset\n",
    "X_test = np.vstack((X1, X2))\n",
    "y_test = np.vstack((y1, y2)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "5b586b79-edbd-4e06-b281-efe8f0bf5023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8000, 3), (8000,), (2000, 3), (2000,))"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "deac64ef-8985-4bb7-b447-c2658b441dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.000e+00 3.904e+03 5.660e+02]\n",
      " [1.000e+00 8.640e+02 1.730e+02]\n",
      " [1.000e+00 4.036e+03 4.110e+02]\n",
      " [1.000e+00 3.120e+03 1.300e+02]\n",
      " [1.000e+00 3.038e+03 8.400e+01]]\n",
      "\n",
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[:5])\n",
    "print()\n",
    "print(y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "edaf8151-95a2-48c7-8142-0be8d310b59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.000e+00 7.460e+02 1.360e+02]\n",
      " [1.000e+00 3.328e+03 3.320e+02]\n",
      " [1.000e+00 1.248e+03 3.400e+01]\n",
      " [1.000e+00 3.659e+03 5.910e+02]\n",
      " [1.000e+00 5.770e+02 2.200e+01]]\n",
      "\n",
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(X_test[:5])\n",
    "print()\n",
    "print(y_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11abc1da-f7b8-4d63-8d73-7d99f09b9547",
   "metadata": {},
   "source": [
    "## Logistics Regression\n",
    "\n",
    "TODO: Brief about logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "007dc18b-0b07-4c8c-b06c-023eeaf35847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train logistic regression on training-set\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6771679-3a73-4a2b-82cb-8fa989551ac6",
   "metadata": {},
   "source": [
    "### Evaluate logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "7ee4b2d7-b4b6-4015-8fa5-1e43998ec8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.]\n",
      "\n",
      "[[4.41640662e-03 9.95583593e-01]\n",
      " [1.52036161e-11 1.00000000e+00]\n",
      " [2.70140879e-05 9.99972986e-01]\n",
      " [1.11572973e-11 1.00000000e+00]\n",
      " [6.12470933e-03 9.93875291e-01]\n",
      " [2.25344147e-02 9.77465585e-01]\n",
      " [6.16168203e-03 9.93838318e-01]\n",
      " [2.88291613e-11 1.00000000e+00]\n",
      " [4.49604798e-11 1.00000000e+00]\n",
      " [1.23192567e-11 1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Predict on test-set and view probabilities\n",
    "pred_proba = lr_model.predict_proba(X_test)\n",
    "print(lr_model.classes_)\n",
    "print()\n",
    "print(pred_proba[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e1497d-11d0-4d0b-b554-170bb1058ab4",
   "metadata": {},
   "source": [
    "Model's predict_proba method outputs probabilities of negative class (0) and positive class (1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "50de11e7-aafd-4711-9488-adddb0313cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict on test-set and view predicted labels\n",
    "pred_labels = lr_model.predict(X_test)\n",
    "pred_labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38868099-8dc2-4706-9f84-a3cc44dd386c",
   "metadata": {},
   "source": [
    "Model's predict method outputs predicted lables of each data point by looking at probabilities of postive class (1) with a default threshold of 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "193f1f38-8f1c-41b0-8c9f-3db1d6260f81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.994"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Measure performance as the overall accuracy of prediction\n",
    "accuracy_score(y_test, pred_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e63ae7c-ae12-4eaa-8c48-eb34dc12b454",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "TODO: Brief about Naive Bayes with formula and explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "fa544d2b-67e8-4863-965f-2363b939e65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier():\n",
    "    def __init__(self, pos_freq_dict, neg_freq_dict):\n",
    "        self.pos_freq_dict = pos_freq_dict\n",
    "        self.neg_freq_dict = neg_freq_dict\n",
    "        self.log_prior = 0\n",
    "        self.logliklyhood_dict = {}\n",
    "        \n",
    "    def fit(self, train_tweets, y_train):\n",
    "        # Compute prior and logprior...\n",
    "        # Calculate total positive and negative tweets \n",
    "        D_pos = len(y_train[y_train == 1.0])\n",
    "        D_neg = len(y_train[y_train == 0.0])\n",
    "        D = D_pos + D_neg\n",
    "        # Calculate probability of tweet being positive or negative\n",
    "        P_pos = D_pos/D\n",
    "        P_neg = D_neg/D\n",
    "        # Calculate prior as ratio of positive and negative probabilities\n",
    "        prior = P_pos/P_neg\n",
    "        # Calculate the log prior\n",
    "        self.log_prior = np.log(prior)\n",
    "        # Compute liklyhood and logliklyhood for each word in our vocabulary...\n",
    "        # Calculate number of unique words, this will be used in smoothing the probability\n",
    "        # to make sure we do not get any zero probability value\n",
    "        vocab = set(train_tweets)\n",
    "        N_unique = len(vocab)\n",
    "        # Claculate total positive and negative frequency\n",
    "        N_pos = sum(pos_freq_dict.values())\n",
    "        N_neg = sum(neg_freq_dict.values())\n",
    "        # Calculate logliklyhood for each word in the vocab\n",
    "        logliklyhood_dict = {}\n",
    "        for word in vocab:\n",
    "            pos_freq = pos_freq_dict.get(word, 0)\n",
    "            neg_freq = neg_freq_dict.get(word, 0)\n",
    "            pos_smooth_proba = (pos_freq + 1)/(N_pos + N_unique)    \n",
    "            neg_smooth_proba = (neg_freq + 1)/(N_neg + N_unique)\n",
    "            liklyhood = pos_smooth_proba/neg_smooth_proba\n",
    "            logliklyhood = np.log(liklyhood)\n",
    "            self.logliklyhood_dict[word] = logliklyhood\n",
    "        \n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        y_pred = []\n",
    "        for tweet in X_test:\n",
    "            # Initial value is log prior\n",
    "            log_liklyhood = self.log_prior\n",
    "            for word in tweet:\n",
    "                # sumup the log liklyhood of the each word\n",
    "                log_liklyhood += self.logliklyhood_dict.get(word, 0)\n",
    "            score = 1.0 if log_liklyhood > 0 else 0.0\n",
    "            y_pred.append(score)\n",
    "        return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "b3ac0f09-2222-4418-9920-244fceed7967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Naive Bayes training we need actual tweet tokens and not the training feature matrix (X_train)\n",
    "# flatten the processed positive and negative tweets and combine them together\n",
    "train_tweet_tokens = list(itertools.chain(*train_pos_tweets)) + list(itertools.chain(*train_neg_tweets))\n",
    "# Train Naive Bayes model on training-set\n",
    "nb_model = NaiveBayesClassifier(pos_freq_dict, neg_freq_dict)\n",
    "nb_model.fit(train_tweet_tokens, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b155a9-5f11-49e0-8f5a-45ff9de15b71",
   "metadata": {},
   "source": [
    "### Evaluate Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "cfc0ba54-453e-423a-ae08-dca8edd32b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Naive Bayes testing we need actual tweet tokens and not the testing feature matrix (X_test)\n",
    "test_tweets = test_pos_tweets + test_neg_tweets\n",
    "# Predict on test-set and view predicted labels\n",
    "pred_labels = nb_model.predict(test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "4a7f769d-2e8e-4c73-aaef-72a0a2caa7ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9955"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Measure performance as the overall accuracy of prediction\n",
    "accuracy_score(y_test, pred_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb0d553-2deb-4a6d-a0d9-b6620832fc79",
   "metadata": {},
   "source": [
    "## Test models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "79a17b3a-78df-4738-ad24-93794a3a0b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A madeup raw tweets\n",
    "positive_test_tweet = '''\n",
    "                     the movie http://abc_movie.com is a gem of movies\n",
    "                     really liked it a lot!!! @abcmovie #awesome\n",
    "                     '''\n",
    "negative_test_tweet = '''\n",
    "                     @_sssingh #bad this movie http://xyzmovie.com \n",
    "                     has to rank one of the worst in history of man kind\n",
    "                     '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3ab8a9-43e7-41e3-8e31-696427e7ad5b",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "a7263a1f-6f60-449f-8356-a2dc28a207d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression...\n",
      "\n",
      "                     @_sssingh #bad this movie http://xyzmovie.com \n",
      "                     has to rank one of the worst in history of man kind\n",
      "                     \n",
      "Above tweet is POSITIVE\n",
      "\n",
      "                     @_sssingh #bad this movie http://xyzmovie.com \n",
      "                     has to rank one of the worst in history of man kind\n",
      "                     \n",
      "Above tweet is NEGATIVE\n"
     ]
    }
   ],
   "source": [
    "# Predict sentiment using logistic regression\n",
    "print('Logistic Regression...')\n",
    "print(negative_test_tweet)\n",
    "sentiment = utils.get_sentiment(positive_test_tweet, lr_model)\n",
    "print(f'Above tweet is {sentiment}')\n",
    "print(negative_test_tweet)\n",
    "sentiment = utils.get_sentiment(negative_test_tweet, lr_model)\n",
    "print(f'Above tweet is {sentiment}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4198cc-3309-4676-a195-df0045542192",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "d24998b4-6e6b-4640-8593-fb00eba5dd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes...\n",
      "\n",
      "                     the movie http://abc_movie.com is a gem of movies\n",
      "                     really liked it a lot!!! @abcmovie #awesome\n",
      "                     \n",
      "Above tweet is POSITIVE\n",
      "\n",
      "                     @_sssingh #bad this movie http://xyzmovie.com \n",
      "                     has to rank one of the worst in history of man kind\n",
      "                     \n",
      "Above tweet is NEGATIVE\n"
     ]
    }
   ],
   "source": [
    "# Predict sentiment using Naive Bayes\n",
    "print('Naive Bayes...')\n",
    "print(positive_test_tweet)\n",
    "sentiment = utils.get_sentiment(positive_test_tweet, nb_model, encode_tweet=False)\n",
    "print(f'Above tweet is {sentiment}')\n",
    "print(negative_test_tweet)\n",
    "sentiment = utils.get_sentiment(negative_test_tweet, nb_model, encode_tweet=False)\n",
    "print(f'Above tweet is {sentiment}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
